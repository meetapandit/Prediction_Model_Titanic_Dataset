confint(glm2, "nadirpsa", level=0.95)
confint.default(glm1, "nadirpsa", level=0.95)
confint.default(glm2, "nadirpsa", level=0.95)
round(exp(confint.default(glm2, "nadir", level=0.95)), 3)
round(exp(confint.default(glm2, "nadirpsa", level=0.95)), 3)
hist(log(x = psa$nadirpsa))
par(mfrow(1,2))
hist(psa$nadirpsa)
hist(log(x = psa$nadirpsa))
par(mfrow=c(1,2))
hist(psa$nadirpsa)
hist(log(x = psa$nadirpsa))
par(mfrow=c(1,2))
hist(psa$nadirpsa, xlab = "Nadirpsa")
hist(log(x = psa$nadirpsa), xlab = "log nadirpsa")
library(tidyverse)
p.parameter <- 0.8
?rbinom
sequence <- rbinom(10, 1, p.parameter)
likelihood <- function(sequence, p.parameter)
{
likelihood <- 1
for (i in 1:length(sequence))
{
if (sequence[i] == 1)
{
#Probability of success is 0.8
likelihood <- likelihood * p.parameter
}
else
{
#Probability of failure is 0.2
likelihood <- likelihood * (1 - p.parameter)
}
}
return(likelihood)
}
?seq()
possible.p <- seq(0, 1, by = 0.001)
qplot(possible.p,
sapply(possible.p, function (p) {likelihood(sequence, p)}),
geom = 'line',
main = 'Likelihood as a Function of P',
xlab = 'P',
ylab = 'Likelihood')
mle.results <- optimize(function(p) {likelihood(sequence, p)},
interval = c(0, 1),
maximum = TRUE)
mle.results
error.behavior <- data.frame()
for (n in 10:2500)
{
sequence <- rbinom(n, 1, p.parameter)
likelihood.results <- optimize(function(p) {likelihood(sequence, p)},
interval = c(0, 1),
maximum = TRUE)
true.mle <- mean(sequence)
likelihood.error <- true.mle - likelihood.results$maximum
error.behavior <- rbind(error.behavior,
data.frame(N = n,
Error = likelihood.error,
Algorithm = 'Likelihood'))
}
true.mle <- mean(sequence)
error.behavior <- data.frame()
for (n in 10:2500)
{
#Generate the sequence of Bernoulli sample space with the assumed probability, number of successes
sequence <- rbinom(n, 1, p.parameter)
#pass the seqeunce to the likelihood function to generate probability of the entire sequence
#Optimize the likelihood values by using the function and generate the maximum likelihood
likelihood.results <- optimize(function(p) {likelihood(sequence, p)},
interval = c(0, 1),
maximum = TRUE)
#Find the mean of the sample space for 2500 samples is 0.785
true.mle <- mean(sequence)
#Error in the likelihood is true value minus expected maximum value is -0.2143
likelihood.error <- true.mle - likelihood.results$maximum
error.behavior <- rbind(error.behavior,
data.frame(N = n,
Error = likelihood.error,
Algorithm = 'Likelihood'))
}
View(error.behavior)
sequence <- rbinom(2500, 1, p.parameter)
likelihood.results <- optimize(function(p) {likelihood(sequence, p)},
interval = c(0, 1),
maximum = TRUE)
likelihood.results
log.likelihood <- function(sequence, p)
{
log.likelihood <- 0
for (i in 1:length(sequence))
{
if (sequence[i] == 1)
{
log.likelihood <- log.likelihood + log(p)
}
else
{
log.likelihood <- log.likelihood + log(1 - p)
}
}
return(log.likelihood)
}
sequence <- c(0, 1, 1, 1, 1, 1, 1, 0, 1, 0)
possible.p <- seq(0, 1, by = 0.001)
qplot(possible.p,
sapply(possible.p, function (p) {log.likelihood(sequence, p)}),
geom = 'line',
main = 'Log Likelihood as a Function of P',
xlab = 'P',
ylab = 'Log Likelihood')
error.behavior <- data.frame()
for (n in 10:2500)
{
sequence <- rbinom(n, 1, p.parameter)
likelihood.results <- optimize(function(p) {likelihood(sequence, p)},
interval = c(0, 1),
maximum = TRUE)
log.likelihood.results <- optimize(function(p) {log.likelihood(sequence, p)},
interval = c(0, 1),
maximum = TRUE)
true.mle <- mean(sequence)
likelihood.error <- true.mle - likelihood.results$maximum
log.likelihood.error <- true.mle - log.likelihood.results$maximum
error.behavior <- rbind(error.behavior,
data.frame(N = n,
Error = likelihood.error,
Algorithm = 'Likelihood'),
data.frame(N = n,
Error = log.likelihood.error,
Algorithm = 'Log Likelihood'))
}
ggplot(error.behavior, aes(x = N, y = Error)) + geom_line(aes(group = Algorithm, color = Algorithm)) + labs(title = 'Long-Term Error Behavior \n of Two Numerical Approaches', xlab = 'Sample Size', ylab = 'Deviation from True MLE')
library(tidyverse)
library(Sleuth3) # Contains data for problemset
library(UsingR) # Contains data for problemset
library(MASS) # Modern applied statistics functions
data("Sleuth3")
data(Sleuth3)
library(tidyverse)
library(Sleuth3) # Contains data for problemset
install.packages("Sleuth3")
data(Sleuth3)
library(Sleuth3) # Contains data for problemset
data(package = "Sleuth3")
library(Sleuth3) # Contains data for problemset
?Sleuth3
library(help="Sleuth3")
data(ex0724)
data_birthrate <- data.frame(ex0724)
View(ex0724)
lm1 <- lm(year~Demark+Netherlands+Canada+USA, data = data_birthrate)
str(data_birthrate)
lm1 <- lm(Year~Demark+Netherlands+Canada+USA, data = data_birthrate)
lm1 <- lm(Year~Denmark+Netherlands+Canada+USA, data = data_birthrate)
summary(lm1)
lm1 <- lm(Denmark~Year, data = data_birthrate)
summary(lm1)
lm2 <- lm(Netherlands~Year, data = data_birthrate)
summary(lm2)
lm3 <- lm(Canada~Year, data = data_birthrate)
summary(lm3)
lm4 <- lm(USA~Year, data = data_birthrate)
summary(lm4)
c(coef(summary(lm1))["data_birthrate$Year","t value"],
coef(summary(lm1))["data_birthrate$Year","Pr(>|t|)"])
c(coef(summary(lm2))["data_birthrate$Year","t value"],
coef(summary(lm2))["data_birthrate$Year","Pr(>|t|)"])
c(coef(summary(lm3))["data_birthrate$Year","t value"],
coef(summary(lm3))["data_birthrate$Year","Pr(>|t|)"])
c(coef(summary(lm4))["data_birthrate$Year","t value"],
coef(summary(lm4))["data_birthrate$Year","Pr(>|t|)"])
coef(summary(lm1))["data_birthrate$Year","t value"]
summary(lm1)
c(coef(summary(lm1))["Year","t value"],
coef(summary(lm1))["Year","Pr(>|t|)"])
c(coef(summary(lm2))["Year","t value"],
coef(summary(lm2))["Year","Pr(>|t|)"])
c(coef(summary(lm3))["Year","t value"],
coef(summary(lm3))["Year","Pr(>|t|)"])
c(coef(summary(lm4))["Year","t value"],
coef(summary(lm4))["Year","Pr(>|t|)"])
(ttest_1) <- c(coef(summary(lm1))["Year","t value"],
coef(summary(lm1))["Year","Pr(>|t|)"])
c(coef(summary(lm1))["Year","t value"],
coef(summary(lm1))["Year","Pr(>|t|)"])
heightData <- tbl_df(get("father.son"))
install.packages("UsingR")
data(package = "UsingR")
heightData <- tbl_df(get("father.son"))
setwd("~/IMT 573/Problem set 5")
heightData <- tbl_df(get("father.son"))
heightData <- father.son
setwd("C:/Users/meeta/AppData/Local/Temp/RtmpaqJaRv/downloaded_packages")
heightData <- father.son
heightData <- tbl_df(get("father.son"))
install.packages("MASS")
install.packages("MASS")
library(MASS) # Modern applied statistics functions
heightData <- tbl_df(get("father.son"))
library(tidyverse)
heightData <- tbl_df(get("father.son"))
# Import and look at the height data
heightData <- tbl_df(get("father.son"))
heightData <- tbl_df(get(father.son))
data(father.son)
data("father.son")
heightData <- tbl_df(get("father.son"))
heightData <- data.frame(father.son)
data(father.son)
head(heightData)
library(tidyverse)
library(Sleuth3) # Contains data for problemset
library(UsingR) # Contains data for problemset
library(MASS) # Modern applied statistics functions
heightData <- tbl_df(get("father.son"))
heightData <- data.frame(father.son)
data(father.son)
?UsingR
??UsingR
data(package = "UsingR")
heightData <- data.frame(father.son)
library(tidyverse)
library(UsingR) # Contains data for problemset
heightData <- tbl_df(get("father.son"))
library(tidyverse)
library(Sleuth3) # Contains data for problemset
library(UsingR) # Contains data for problemset
library(MASS) # Modern applied statistics functions
heightData <- tbl_df(get("father.son"))
?UsingR
library("data.table", lib.loc="~/R/win-library/3.3")
library("UsingR", lib.loc="~/R/win-library/3.3")
install.packages("sp")
install.packages("rgeos")
install.packages("maptools")
install.packages("spdep")
install.packages("devtools")
install.packages("rgeos")
install.packages("maptools")
install.packages("spdep")
install.packages("devtools")
install.packages("rgdal")
install.packages("UScensus2010", repos="http://R-Forge.R-project.org")
install.packages("UScensus2010county", repos="http://R-Forge.R-project.org")
install.packages("UScensus2010tract", repos="http://R-Forge.R-project.org")
install.packages("UScensus2010cdp", repos="http://R-Forge.R-project.org")
setwd("~/IMT 573/Problem set 7")
# Load standard libraries
library(tidyverse)
library(gridExtra)
library(MASS)
library(pROC)
install.packages("pROC")
library(arm)
install.packages("arm")
library(arm)
install.packages("randomforest")
library(randomForest)
install.packages("randomForest")
library(randomForest)
install.packages("xgboost")
library(xgboost)
titanic_data <- read.csv('titanic.csv')
View(titanic_data)
str(titanic_data) # explore data structure
smp_size <- floor(0.8 * nrow(titanic_data))
set.seed(123)
train_ind <- sample(seq_len(nrow(titanic_data)), size = smp_size)
train <- titanic_data[train_ind, ]
test <- titanic_data[-train_ind, ]
glm1 <- glm(survived~pclass, data = train, family = "binomial")
View(train)
View(test)
glm1 <- glm(survived~pclass+fare, data = train, family = "binomial")
summary(glm1)
bayesglm1 <- bayesglm(survived~pclass+fare, data = train, family = "binomial")
summary(bayesglm1)
str(titanic_data) # explore data structure
titanic_data$pclass <- as.factor(titanic_data$pclass)
View(titanic_data)
str(titanic_data)
smp_size <- floor(0.8 * nrow(titanic_data))
set.seed(123)
train_ind <- sample(seq_len(nrow(titanic_data)), size = smp_size)
train <- titanic_data[train_ind, ]
test <- titanic_data[-train_ind, ]
glm1 <- glm(survived~pclass+fare, data = train, family = "binomial")
summary(glm1)
bayesglm1 <- bayesglm(survived~pclass+fare, data = train, family = "binomial")
summary(bayesglm1)
yhat <- predict.glm(glm1, newdata = train, type = "response")
yhat <- predict.glm(glm1, newdata = test, type = "response")
yhat <- ifelse(yhat > 0.5,1,0)
table(test$survived,yhat)
145+32/262
table(yhat, test$survived)
145+32/262
145+32
177/232
(145+32)/262
roc(test$survived,yhat)
library(pROC)
roc(test$survived,yhat)
plot(roc(test$survived,yhat))
str(titanic_data)
titanic_new <- strsplit(titanic_data$name, ",.")
titanic_data$name <- as.character(titanic_data$name)
titanic_new <- strsplit(titanic_data$name, ",.")
titanic_new$title <- sapply(titanic_data$name,
FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
titanic_data$title <- sapply(titanic_data$name,
FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
View(titanic_data)
titanic_temp <- titanic_new %>%
group_by(title) %>%
summarise(n())
titanic_temp <- titanic_data %>%
group_by(title) %>%
summarise(n())
View(titanic_temp)
plot(roc(test$survived,yhat), col= "blue")
titanic_data$title[titanic_data$title %in% c('Mme', 'Mlle', 'Ms')] <- 'Miss'
titanic_data$title[titanic_data$title %in% c('Capt', 'Col', 'Don','Dr', 'Major', 'Sir', 'Rev')] <- 'Sir'
titanic_data$title[titanic_data$title %in% c('Dona', 'Jonkheer', 'Lady', 'the Countess')] <- 'Lady'
titanic_data$title <- as.factor(titanic_data$title)
View(titanic_data)
titanic_data$title[titanic_data$title %in% c('Mme', 'Mlle', 'Ms')] <- 'Miss'
titanic_data$title[titanic_data$title %in% c('Capt', 'Col', 'Don','Dr', 'Major', 'Sir', 'Rev')] <- 'Sir'
titanic_data$title[titanic_data$title %in% c('Dona', 'Jonkheer', 'Lady', 'the Countess')] <- 'Lady'
titanic_temp <- titanic_data %>%
group_by(title) %>%
summarise(n())
titanic_data$title <- as.factor(titanic_data$title)
View(titanic_data)
titanic_data$title <- sub(' ', '', titanic_data$title)
View(titanic_data)
titanic_data$title[titanic_data$title %in% c('Mme', 'Mlle', 'Ms')] <- 'Miss'
titanic_data$title[titanic_data$title %in% c('Capt', 'Col', 'Don','Dr', 'Major', 'Sir', 'Rev')] <- 'Sir'
titanic_data$title[titanic_data$title %in% c('Dona', 'Jonkheer', 'Lady', 'the Countess')] <- 'Lady'
titanic_data$title <- as.factor(titanic_data$title)
smp_size <- floor(0.8 * nrow(titanic_data))
set.seed(123)
train_index <- sample(seq_len(nrow(titanic_data)), size = smp_size)
train1 <- titanic_data[train_index, ]
test1 <- titanic_data[-train_index, ]
bayesglm2<- bayesglm(survived~pclass+fare+title, data = train1, family = "binomial")
summary(bayesglm2)
yhat2 <- predict.glm(bayesglm2, newdata = test1, type = "response")
yhat2 <- ifelse(yhat2 > 0.5,1,0)
table(yhat2, test1$survived)
(138+61)/262
predict_train1 <- predict.glm(bayesglm2, newdata = train1, type = "response")
predict_train1 <- ifelse(predict_train1 > 0.5,1,0)
table(predict_train1, train1$survived)
(542+293)/1047
table(yhat2, test1$survived)
table(predict_train1, train1$survived)
?randomForest
set.seed(1)
rm1 <- randomForest(as.factor(survived)~pclass+title, data = train1, ntree = 500, importance= T)
summary(rm1)
yhat3 <- predict(rm1, test1, type="response")
rm2 <- randomForest(as.factor(survived)~pclass+title+age+sex+fare, data = train1, ntree = 500, importance= T)
set.seed(1)
rm2 <- randomForest(as.factor(survived)~pclass+title+age+sex+fare, data = train1, ntree = 500, importance= T)
set.seed(111)
rm2 <- randomForest(as.factor(survived)~pclass+title+age+sex+fare, data = train1, ntree = 500, importance= T)
set.seed(111)
rm2 <- randomForest(as.factor(survived)~pclass+title+fare, data = train1, ntree = 500, importance= T)
rm2 <- randomForest(as.factor(survived)~pclass+title+as.factor(sex), data = train1, ntree = 500, importance= T)
View(train1)
set.seed(111)
rm2 <- randomForest(as.factor(survived)~pclass+title+as.factor(sex), data = train1, ntree = 500, importance= T)
set.seed(111)
rm2 <- randomForest(as.factor(survived)~pclass+title+sex, data = train1, ntree = 500, importance= T)
summary(rm2)
yhat3 <- predict(rm2, test1, type="response")
yhat3 <- predict(rm1, test1, type="response")
yhat4 <- predict(rm2, test1, type="response")
str(train1)
plot(roc(test1$survived,yhat3), col= "red")
plot(roc(test1$survived,yhat3), col= "red")
set.seed(1)
rm1 <- randomForest(survived~pclass+title, data = train1, ntree = 500, importance= T)
summary(rm1)
yhat3 <- predict(rm1, test1, type="response")
set.seed(1)
rm1 <- randomForest(survived~pclass+title, data = train1, ntree = 500, importance= T)
summary(rm1)
yhat3 <- predict(rm1, test1, type="response")
rm2 <- randomForest(survived~pclass+title+sex, data = train1, ntree = 500, importance= T)
set.seed(111)
rm2 <- randomForest(survived~pclass+title+sex, data = train1, ntree = 500, importance= T)
summary(rm2)
yhat4 <- predict(rm2, test1, type="response")
plot(roc(test1$survived,yhat3), col= "red")
plot(roc(test1$survived,yhat4), col= "forestgreen")
plot(roc(test1$survived,yhat3), col= "red")
plot(roc(test1$survived,yhat4), col= "forestgreen")
library(Matrix)
sparse.matrix.train <- sparse.model.matrix(survived~pclass + sex + title -1, data = train1) #converts train set factors to columns
sparse.matrix.test <-  sparse.model.matrix(survived~pclass + sex + title -1, data = test1) #converts test set factors to columns
output_vector = train$survived #output vector to be predicted
xgb.model.one <- xgb.cv(data= sparse.matrix.train,     #train sparse matrix
label= output_vector,    #output vector to be predicted
eval.metric = 'logloss',     #model minimizes Root Mean Squared Error
objective = "reg:logistic", #regression
nfold = 10,
#tuning parameters
max.depth = 3,            #Vary btwn 3-15
eta = 0.05,                #Vary btwn 0.1-0.3
nthread = 5,             #Increase this to improve speed
subsample= 1,            #Vary btwn 0.8-1
colsample_bytree = 0.5,   #Vary btwn 0.3-0.8
lambda = 0.5,             #Vary between 0-3
alpha = 0.5,              #Vary between 0-3
min_child_weight = 3,     #Vary btwn 1-10
nround = 100             #Vary btwn 100-3000 based on max.depth, eta,subsample & colsample
)
plot(data.frame(xgb.model.one)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.one)[,3], type='l', lty=2, col='black')
xgb.model.one <- xgb.cv(data= sparse.matrix.train,     #train sparse matrix
label= output_vector,    #output vector to be predicted
eval.metric = 'logloss',     #model minimizes Root Mean Squared Error
objective = "reg:logistic", #regression
nfold = 10,
#tuning parameters
max.depth = 8,            #Vary btwn 3-15
eta = 0.1,                #Vary btwn 0.1-0.3
nthread = 5,             #Increase this to improve speed
subsample= 1,            #Vary btwn 0.8-1
colsample_bytree = 0.5,   #Vary btwn 0.3-0.8
lambda = 0.5,             #Vary between 0-3
alpha = 0.5,              #Vary between 0-3
min_child_weight = 3,     #Vary btwn 1-10
nround = 100             #Vary btwn 100-3000 based on max.depth, eta,subsample & colsample
)
plot(data.frame(xgb.model.one)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.one)[,3], type='l', lty=2, col='black')
xgb.model.one <- xgb.cv(data= sparse.matrix.train,     #train sparse matrix
label= output_vector,    #output vector to be predicted
eval.metric = 'logloss',     #model minimizes Root Mean Squared Error
objective = "reg:logistic", #regression
nfold = 10,
#tuning parameters
max.depth = 3,            #Vary btwn 3-15
eta = 0.05,                #Vary btwn 0.1-0.3
nthread = 5,             #Increase this to improve speed
subsample= 1,            #Vary btwn 0.8-1
colsample_bytree = 0.5,   #Vary btwn 0.3-0.8
lambda = 0.5,             #Vary between 0-3
alpha = 0.5,              #Vary between 0-3
min_child_weight = 3,     #Vary btwn 1-10
nround = 100             #Vary btwn 100-3000 based on max.depth, eta,subsample & colsample
)
plot(data.frame(xgb.model.one)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.one)[,3], type='l', lty=2, col='black')
library(Matrix)
sparse.matrix.train <- sparse.model.matrix(survived~pclass + sex + title -1, data = train1) #converts train set factors to columns
sparse.matrix.test <-  sparse.model.matrix(survived~pclass + sex + title -1, data = test1) #converts test set factors to columns
output_vector = train1$survived #output vector to be predicted
xgb.model.one <- xgb.cv(data= sparse.matrix.train,     #train sparse matrix
label= output_vector,    #output vector to be predicted
eval.metric = 'logloss',     #model minimizes Root Mean Squared Error
objective = "reg:logistic", #regression
nfold = 10,
#tuning parameters
max.depth = 3,            #Vary btwn 3-15
eta = 0.05,                #Vary btwn 0.1-0.3
nthread = 5,             #Increase this to improve speed
subsample= 1,            #Vary btwn 0.8-1
colsample_bytree = 0.5,   #Vary btwn 0.3-0.8
lambda = 0.5,             #Vary between 0-3
alpha = 0.5,              #Vary between 0-3
min_child_weight = 3,     #Vary btwn 1-10
nround = 100             #Vary btwn 100-3000 based on max.depth, eta,subsample & colsample
)
plot(data.frame(xgb.model.one)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.one)[,3], type='l', lty=2, col='black')
xgb.model.two <- xgb.cv(data= sparse.matrix.train,     #train sparse matrix
label= output_vector,    #output vector to be predicted
eval.metric = 'logloss',     #model minimizes Root Mean Squared Error
objective = "reg:logistic", #regression
nfold = 10,
#tuning parameters
max.depth = 8,            #Vary btwn 3-15
eta = 0.1,                #Vary btwn 0.1-0.3
nthread = 5,             #Increase this to improve speed
subsample= 1,            #Vary btwn 0.8-1
colsample_bytree = 0.5,   #Vary btwn 0.3-0.8
lambda = 0.5,             #Vary between 0-3
alpha = 0.5,              #Vary between 0-3
min_child_weight = 3,     #Vary btwn 1-10
nround = 100             #Vary btwn 100-3000 based on max.depth, eta,subsample & colsample
)
plot(data.frame(xgb.model.two)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.two)[,3], type='l', lty=2, col='black')
plot(data.frame(xgb.model.one)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.one)[,3], type='l', lty=2, col='black')
lines(data.frame(xgb.model.two)[,3], type='l', lty=3, col='black')
plot(data.frame(xgb.model.one)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.one)[,3], type='l', lty=2, col='red')
lines(data.frame(xgb.model.two)[,3], type='l', lty=3, col='forestgreen')
plot(data.frame(xgb.model.one)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.one)[,3], type='l', lty=2, col='red')
lines(data.frame(xgb.model.two)[,3], type='l', lty=4, col='forestgreen')
plot(data.frame(xgb.model.one)[,1], type='l', col='black', ylab='CV logloss Error', xlab='# of trees')
lines(data.frame(xgb.model.one)[,3], type='l', lty=2, col='red')
lines(data.frame(xgb.model.two)[,3], type='l', lty=6, col='forestgreen')
